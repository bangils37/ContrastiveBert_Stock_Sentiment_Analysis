{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1776895,"sourceType":"datasetVersion","datasetId":1056391}],"dockerImageVersionId":30043,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"vezpsX-7GphM","outputId":"a7164402-8a7b-4e4c-e118-ce6498ba4f2e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\nfrom collections import defaultdict\n\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"WtQykqrfEzUT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set intial variables and constants\n%config InlineBackend.figure_format='retina'\n\n# Graph Designs\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\n\n# Random seed for reproducibilty\n# RANDOM_SEED = 42\n# np.random.seed(RANDOM_SEED)\n# torch.manual_seed(RANDOM_SEED)\n\n# Set GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"gRINjFWWEzUb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gdown --quiet  \n\nimport gdown\nimport os\n\ndef downloadFiles(file_urls, target_dir):\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n    \n    for file_name, file_url in file_urls:\n        file_path = os.path.join(target_dir, file_name)\n        print(f\"Đang tải {file_name} vào {target_dir}...\")\n        gdown.download(file_url, file_path, quiet=False)\n\nfile_list = [\n    [\"train_stockemo.csv\", \"https://drive.google.com/uc?id=14kpQhdpjt57ySe9omZSofbmFF4iYUIDc\"],\n    [\"val_stockemo.csv\", \"https://drive.google.com/uc?id=1-8FC0f1RDCNSkRt8doTDMAPrmdmazQ5u\"],\n    [\"test_stockemo.csv\", \"https://drive.google.com/uc?id=1-A1n7mRMbje-me1rQpce_QsfFnlH0av7\"],\n    [\"processed_stockemo.csv\", \"https://drive.google.com/uc?id=1-7QLxjVIezZLJ_Og5m3DmmW32BabnH2i\"]\n]\n\ndownloadFiles(file_list, \"stockemo\")\n\nprint(\"\\nDanh sách file trong folder 'stockemo':\")\nprint(os.listdir(\"stockemo\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/working/stockemo/train_stockemo.csv')\ndf_test = pd.read_csv('/kaggle/working/stockemo/test_stockemo.csv')\ndf_val = pd.read_csv('/kaggle/working/stockemo/val_stockemo.csv')\ndf = pd.concat([df_train, df_val, df_test], axis=0, ignore_index=True)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\ndf_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\ndf_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)\ndf_val = df_val.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(df_train.shape)\nprint(df_test.shape)\nprint(df_val.shape)\nprint(df.shape)","metadata":{"id":"g6b5ajqzEzUg","outputId":"a7888891-5d69-42b7-a1e0-ccdbe75f04e4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"id":"eQ5Uwg8xEzUk","outputId":"2b23324b-d208-462f-d011-95c2f7847390","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"id":"24xNLIJlEzUo","outputId":"eae6af5f-068a-480e-acdd-3c78b3e8a81b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def to_sentiment_label(value):\n    if value == 'bullish':\n        return 1\n    else:\n        return 0\n\ndef to_emotion_label(value):\n    emotion_mapping = {\n        'ambiguous': 0,  # Mơ hồ (tiêu cực nhất)\n        'anxiety': 1,    # Lo lắng\n        'panic': 2,      # Hoảng loạn\n        'depression': 3, # Trầm cảm\n        'confusion': 4,  # Bối rối\n        'anger': 5,      # Tức giận\n        'disgust': 6,    # Ghê tởm\n        'surprise': 7,   # Ngạc nhiên\n        'belief': 8,     # Niềm tin\n        'optimism': 9,   # Lạc quan\n        'excitement': 10,# Hào hứng\n        'amusement': 11  # Thích thú (tích cực nhất)\n    }\n\n    return emotion_mapping.get(value, -1)  # -1 nếu không hợp lệ\n\ndef remove_first_word(value):\n    \"\"\"\n    Loại bỏ chữ đầu tiên trong chuỗi và trả về chuỗi còn lại.\n\n    Parameters:\n        value (str): Chuỗi đầu vào.\n\n    Returns:\n        str: Chuỗi sau khi đã bỏ chữ đầu tiên.\n    \"\"\"\n    # Tách chuỗi thành danh sách các từ\n    words = value.split()\n    \n    # Kiểm tra nếu chuỗi có ít nhất 2 từ\n    if len(words) > 1:\n        # Ghép lại các từ còn lại\n        return \" \".join(words[1:])\n    else:\n        # Nếu chỉ có 1 từ hoặc chuỗi rỗng, trả về chuỗi rỗng\n        return \"\"\n\n# Áp dụng hàm to_sentiment_label và to_emotion_label vào các cột của tập dữ liệu\ndf_train['sentiment_label'] = df_train.senti_label.apply(to_sentiment_label)\ndf_train['emotion_label'] = df_train.emo_label.apply(to_emotion_label)\n#df_train['content'] = df_train.processed.apply(remove_first_word)\n\ndf_test['sentiment_label'] = df_test.senti_label.apply(to_sentiment_label)\ndf_test['emotion_label'] = df_test.emo_label.apply(to_emotion_label)\n#df_test['content'] = df_test.processed.apply(remove_first_word)\n\ndf_val['sentiment_label'] = df_val.senti_label.apply(to_sentiment_label)\ndf_val['emotion_label'] = df_val.emo_label.apply(to_emotion_label)\n#df_val['content'] = df_val.processed.apply(remove_first_word)\n\ndf['sentiment_label'] = df.senti_label.apply(to_sentiment_label)\ndf['emotion_label'] = df.emo_label.apply(to_emotion_label)\n#df['content'] = df.processed.apply(remove_first_word)","metadata":{"id":"MyK4bEuhEzUy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the distribution\nclass_sentiments = [\n    'negative', 'positive'\n]\nclass_emotions = [\n    'ambiguous', 'anxiety', 'panic', 'depression', 'confusion', \n    'anger', 'disgust', 'surprise', 'belief', 'optimism', \n    'excitement', 'amusement'\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = 'bert-base-cased'\n\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\ntokenizer","metadata":{"id":"Uj5eXyqYEzU6","outputId":"4a6b71ec-7503-4884-c669-08383d60225e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_LEN = 160","metadata":{"id":"blPAZJOPEzVJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StockEmoDataset(Dataset):\n    def __init__(self, contents, targets, tokenizer, max_len):\n        self.contents = contents\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.contents)\n\n    def __getitem__(self, item):\n        content = str(self.contents[item])\n        target = self.targets[item]\n\n        encoding = self.tokenizer.encode_plus(\n            content,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        return {\n            'content_text': content,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets': torch.tensor(target, dtype=torch.long)\n        }\n        \n#Example\nexample_text = \"Hello, how are you?\"\n\nencoding = tokenizer.encode_plus(\n    example_text,\n    add_special_tokens=True,  # Thêm [CLS] ở đầu và [SEP] ở cuối\n    max_length=10,            # Giới hạn độ dài là 10 token\n    return_token_type_ids=False,\n    pad_to_max_length=True,   # Thêm [PAD] nếu cần để đạt đến độ dài 10\n    return_attention_mask=True,\n    return_tensors='pt'       # Trả về tensor PyTorch\n)\n\nprint(encoding)","metadata":{"id":"Sbakw3KIEzVM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = StockEmoDataset(\n        contents=df.processed.to_numpy(),\n        targets=df.emotion_label.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len\n    )\n    \n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        num_workers=0\n    )","metadata":{"id":"c7SCbUlmEzVT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create train, test and val data loaders\nBATCH_SIZE = 64\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)","metadata":{"id":"31-GApKIEzVW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert_model = BertModel.from_pretrained(MODEL_NAME)","metadata":{"id":"1oVuK0UUEzVg","outputId":"354aaaea-b336-41cc-a1de-8a5583545f17","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    def __init__(self, hidden_dim, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(MODEL_NAME)\n        self.out = nn.Sequential(\n            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(hidden_dim, n_classes)\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        logits = self.out(pooled_output)\n        \n        return logits, pooled_output  ","metadata":{"id":"v98I1vshEzVi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = SentimentClassifier(512, len(class_emotions))\nmodel = model.to(device)","metadata":{"id":"5eh4ytjBEzVm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(bert_model.config.hidden_size)","metadata":{"id":"iFkXhnrLEzVr","outputId":"f065e9d5-9ca5-4a7e-9c20-e953d5da05f1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 5\n\n# Optimizer Adam \noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n\n# Set the loss function \nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"id":"CEQfhsdnEzVw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def supervised_simcse_loss(embeddings, labels, temperature=0.05):\n    \"\"\"\n    Compute the supervised SimCSE loss.\n    \n    Args:\n        embeddings (torch.Tensor): Tensor of shape (batch_size, hidden_dim) containing sentence embeddings.\n        labels (torch.Tensor): Tensor of shape (batch_size,) containing the labels for supervised contrastive learning.\n        temperature (float): Temperature scaling parameter for contrastive loss.\n    \n    Returns:\n        torch.Tensor: The computed contrastive loss.\n    \"\"\"\n    # Normalize embeddings to unit vectors\n    embeddings = F.normalize(embeddings, p=2, dim=1)\n    \n    # Compute similarity matrix (batch_size x batch_size)\n    similarity_matrix = torch.matmul(embeddings, embeddings.T)  # cosine similarity\n    \n    # Scale by temperature\n    similarity_matrix = similarity_matrix / temperature\n    \n    # Create labels for the contrastive loss\n    batch_size = labels.size(0)\n    contrastive_labels = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float()  # shape: (batch_size, batch_size)\n    \n    # Mask diagonal (self-comparisons should not contribute to loss)\n    mask = ~torch.eye(batch_size, dtype=bool, device=labels.device)\n    \n    # Apply mask and compute log-softmax\n    similarity_matrix_exp = torch.exp(similarity_matrix)\n    masked_similarity = similarity_matrix_exp * mask\n    log_prob = similarity_matrix - torch.log(masked_similarity.sum(dim=1, keepdim=True))\n    \n    # Compute supervised contrastive loss\n    contrastive_loss = - (contrastive_labels * log_prob).sum(dim=1) / contrastive_labels.sum(dim=1)\n    \n    # Average over the batch\n    loss = contrastive_loss.mean()\n    \n    return loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples, temperature=0.05):\n    model = model.train()\n    classification_losses = []\n    contrastive_losses = []\n    correct_predictions = 0\n    \n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        \n        # Forward pass to get logits and embeddings\n        logits, embeddings = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Classification loss\n        _, preds = torch.max(logits, dim=1)\n        classification_loss = loss_fn(logits, targets)\n        \n        # Contrastive loss\n        contrastive_loss = supervised_simcse_loss(embeddings, targets, temperature=temperature)\n        \n        # Combine both losses\n        loss = classification_loss + contrastive_loss\n        \n        correct_predictions += torch.sum(preds == targets)\n        classification_losses.append(classification_loss.item())\n        contrastive_losses.append(contrastive_loss.item())\n        \n        # Backward prop\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    \n    return (\n        correct_predictions.double() / n_examples,\n        np.mean(classification_losses),\n        np.mean(contrastive_losses),\n    )","metadata":{"id":"jYmCvwhmEzVz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            \n            # Forward pass\n            logits, _ = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            \n            # Classification loss\n            _, preds = torch.max(logits, dim=1)\n            loss = loss_fn(logits, targets)\n            \n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n    \n    return correct_predictions.double() / n_examples, np.mean(losses)","metadata":{"id":"Yil2jCZfEzV2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    \n    # Show details \n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(\"-\" * 10)\n    \n    # Train the model\n    train_acc, train_class_loss, train_contrast_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(df_train),\n        temperature=0.05\n    )\n    \n    print(f\"Train loss {train_class_loss:.4f} accuracy {train_acc:.4f}\")\n    print(f\"Classification Loss {train_class_loss:.4f}\")\n    print(f\"Contrastive Loss {train_contrast_loss:.4f}\")\n    \n    # Get model performance (accuracy and loss)\n    val_acc, val_loss = eval_model(\n        model,\n        val_data_loader,\n        loss_fn,\n        device,\n        len(df_val)\n    )\n    \n    print(f\"Val loss {val_loss:.4f} accuracy {val_acc:.4f}\")\n    print()\n    \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_class_loss)\n    history['train_contrast_loss'].append(train_contrast_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    # If we beat previous performance, save the model\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc\n\n    #Batch_size = 64\n    #drop_out = 0.5","metadata":{"id":"tz0rylI3EzV5","outputId":"545b8419-84b1-4b11-af85-1c45bdc118d9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and validation accuracy\nplt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\n\n# Graph chars\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","metadata":{"id":"mkOsxRIcHgKt","outputId":"11116244-c94e-43de-f9c5-58dff0c7cc75","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\n\ntest_acc.item()","metadata":{"id":"q53eNoW2IS7C","outputId":"62b82769-6859-49b8-89c3-3fad47b76763","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_predictions(model, data_loader):\n    model = model.eval()\n\n    review_texts = []\n    predictions = []\n    prediction_probs = []\n    real_values = []\n\n    with torch.no_grad():\n        for d in data_loader:\n            texts = d[\"content_text\"]\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n\n            logits, _ = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            _, preds = torch.max(logits, dim=1)\n\n            review_texts.extend(texts)\n            predictions.extend(preds)\n            prediction_probs.extend(logits)\n            real_values.extend(targets)\n\n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n    real_values = torch.stack(real_values).cpu()\n\n    return review_texts, predictions, prediction_probs, real_values","metadata":{"id":"C2gJhrUHIfsm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n    model,\n    test_data_loader\n)","metadata":{"id":"18mV-rdaIy-T","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(classification_report(y_test, y_pred, target_names=class_emotions))","metadata":{"id":"1uvXvTrQVcK6","outputId":"2f139229-c8e1-4b2d-ffd8-63af9520fc0a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_confusion_matrix(confusion_matrix):\n    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n    plt.ylabel('True sentiment')\n    plt.xlabel('Predicted sentiment');\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=class_emotions, columns=class_emotions)\nshow_confusion_matrix(df_cm)","metadata":{"id":"YWVfnVW3VgL-","outputId":"ffc9aecb-4868-4e3c-d3b4-e374372ec3f1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"review_text = \"I love completing my todos! Best app ever!!!\"","metadata":{"id":"S4CWFF0MEzV9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_review = tokenizer.encode_plus(\n    review_text,\n    max_length=MAX_LEN,\n    add_special_tokens=True,\n    return_token_type_ids=False,\n    pad_to_max_length=True,\n    return_attention_mask=True,\n    return_tensors='pt',\n)","metadata":{"id":"TOelFhlsSiK2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_ids = encoded_review['input_ids'].to(device)\nattention_mask = encoded_review['attention_mask'].to(device)\n\nlogits, _ = model(input_ids, attention_mask)\n_, prediction = torch.max(logits, dim=1)\n\nprint(f'Review text: {review_text}')\nprint(f'Sentiment  : {class_emotions[prediction]}')","metadata":{"id":"3LZm3Ag0WGpi","outputId":"adc38462-7a4a-4554-c856-f8af32c3c8c2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}